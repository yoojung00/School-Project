{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5328 Assignment 2 - Label noise classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Load packages](#packages)\n",
    "2. [Load datasets](#datasets)\n",
    "3. [Pre-processing](#preprocessing)\n",
    "4. [Experiments](#experiments)\n",
    "    - 4.1 [Baseline](#baseline) \n",
    "    - 4.2 [Importance Reweighting](#rw_method1)\n",
    "    - 4.3 [Reweighting method by Nagarajan Natarajan](#rw_method2)\n",
    "    - 4.4 [Estimate noise rate](#estimate_rho)\n",
    "5. [Reference](#Reference)\n",
    "6. Packages versions\n",
    "    - `python`: 3.7.0\n",
    "    - `sklearn`: 0.19.2\n",
    "    - `numpy`: 1.15.1\n",
    "    - `matplotlib`: 2.2.3\n",
    "    - `PIL`: 5.2.0\n",
    "    - `skimage`: 0.14.1\n",
    "    - `LIBSVM`: libsvm-weights-3.23 (Ths installation and set up can be found in README)\n",
    "    \n",
    "    (**Notice**: if some packages versions (e.g. skimage) are not matched, this may cause problems when run the code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"packages\"></a>\n",
    "## 1. Load packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper libraries\n",
    "import collections\n",
    "import numpy as np\n",
    "import numpy.linalg as lng\n",
    "import time as time\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# LIBSVM (libsvm-weights-3.23)\n",
    "# The installation and setup can be found in README\n",
    "from svmutil import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"datasets\"></a>\n",
    "## 2. Load datasets\n",
    "\n",
    "### NpzFile(fid)\n",
    "\n",
    "A dictionary-like object with lazy-loading of files in the zipped\n",
    "archive provided on construction.\n",
    "\n",
    "`NpzFile` is used to load files in the NumPy ``.npz`` data archive\n",
    "format. It assumes that files in the archive have a ``.npy`` extension,\n",
    "other files are ignored.\n",
    "\n",
    "The arrays and file strings are lazily loaded on either\n",
    "getitem access using ``obj['key']`` or attribute lookup using\n",
    "``obj.f.key``. A list of all files (without ``.npy`` extensions) can\n",
    "be obtained with ``obj.files`` and the ZipFile object itself using\n",
    "``obj.zip``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(_):\n",
    "    # This function is used to load MNIST and CIFAR dataset from NpzFile\n",
    "    dataset = np.load('{}_dataset.npz'.format(_))\n",
    "    Xtr = dataset['Xtr']\n",
    "    Str = dataset['Str'].reshape(len(Xtr),) # Reshape 'Str' for later svm training \n",
    "    Xts = dataset['Xts']\n",
    "    Yts = dataset['Yts'].reshape(len(Xts),)\n",
    "    print('Load {}_dataset successfully'.format(_))\n",
    "    \n",
    "    return Xtr, Str, Xts, Yts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Viewing .npy images https://stackoverflow.com/questions/33480297/viewing-npy-images\n",
    "\n",
    "The raw data are 28 × 28 (for `Fashion-MNIST`) or 32 × 32 × 3 (for `CIFAR`) images, which are reshaped to features with dimension d = 784 or d = 3072. \n",
    "\n",
    "(\\*Note: We just use this step to visulise the input and test if data loading is correct, we do not use any advantages from this step to train our model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('mnist_dataset.npz')\n",
    "\n",
    "x = dataset['Xtr'][0].reshape(28,28)\n",
    "x.shape\n",
    "img = Image.fromarray(x)\n",
    "#img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = np.load('cifar_dataset.npz')\n",
    "\n",
    "#x = dataset['Xtr'][456].reshape(1,-1).reshape(32,32,3)#.transpose()\n",
    "\n",
    "x = dataset['Xtr'][0].reshape(3,-1).T.reshape(32,-1,3)\n",
    "x.shape\n",
    "img = Image.fromarray(x)\n",
    "#img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = dataset['Xtr'][456].reshape(1,-1).reshape(32,32,3)#.transpose()\n",
    "x = dataset['Xtr'][0].reshape(3,-1).T.reshape(32,-1,3)\n",
    "x.shape\n",
    "img = Image.fromarray(x)\n",
    "#img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load CIFAR and MNIST dataset separately.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load cifar_dataset successfully\n"
     ]
    }
   ],
   "source": [
    "x_train_cifar, y_train_cifar,x_test_cifar,y_test_cifar = load_data('cifar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072)\n",
      "(10000,)\n",
      "(2000, 3072)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_cifar.shape)\n",
    "print(y_train_cifar.shape)\n",
    "print(x_test_cifar.shape)\n",
    "print(y_test_cifar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load mnist_dataset successfully\n"
     ]
    }
   ],
   "source": [
    "x_train_mnist,y_train_mnist,x_test_mnist,y_test_mnist = load_data('mnist') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(10000,)\n",
      "(2000, 784)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_mnist.shape)\n",
    "print(y_train_mnist.shape)\n",
    "print(x_test_mnist.shape)\n",
    "print(y_test_mnist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "## 3. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_mnist_reduce = skimage.measure.block_reduce(x_train_mnist, (1,2), np.max)\n",
    "#x_test_mnist_reduce = skimage.measure.block_reduce(x_test_mnist, (1,2), np.max)\n",
    "#print(x_train_mnist_reduce.shape)\n",
    "#print(x_test_mnist_reduce.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    # min-max normalisation\n",
    "    return (x-x.min())/(x.max()-x.min()) \n",
    "\n",
    "def cal_pca(train,test,threshold = 0.95):\n",
    "    '''\n",
    "    input:\n",
    "    train - train data\n",
    "    test - test data\n",
    "    threshold - percentage of variance kept after pca\n",
    "    '''\n",
    "    pca = PCA(threshold)\n",
    "    train -= train.mean()\n",
    "    pca.fit(train)\n",
    "    train_ = pca.transform(train)\n",
    "    test -= test.mean()\n",
    "    test_ = pca.transform(test)\n",
    "    \n",
    "    return train_,test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset train set feature dimensions: (10000, 129)\n",
      "MNIST dataset test set feature dimensions: (2000, 129)\n",
      "MNIST dataset train set label dimensions: (10000,)\n",
      "MNIST dataset test set label dimensions: (2000,)\n",
      "\n",
      "CIFAR dataset train set feature dimensions: (10000, 205)\n",
      "CIFAR dataset test set feature dimensions: (2000, 205)\n",
      "CIFAR dataset test set label dimensions: (10000,)\n",
      "CIFAR dataset test set label dimensions: (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Normalise and dimension reduction of MNIST dataset\n",
    "xtr_mnist,xts_mnist = cal_pca(norm(x_train_mnist),norm(x_test_mnist),0.95)\n",
    "#xtr_mnist,xts_mnist = cal_pca(x_train_mnist,x_test_mnist,0.95)\n",
    "str_mnist,yts_mnist = y_train_mnist,y_test_mnist\n",
    "\n",
    "# Normalise and dimension reduction of CIFAR datset\n",
    "xtr_cifar,xts_cifar = cal_pca(norm(x_train_cifar),norm(x_test_cifar),0.95)\n",
    "#xtr_cifar,xts_cifar = cal_pca(x_train_cifar,x_test_cifar,0.95)\n",
    "str_cifar,yts_cifar = y_train_cifar,y_test_cifar\n",
    "\n",
    "# Print dimensions \n",
    "print('MNIST dataset train set feature dimensions:', xtr_mnist.shape)\n",
    "print('MNIST dataset test set feature dimensions:', xts_mnist.shape)\n",
    "print('MNIST dataset train set label dimensions:', str_mnist.shape)\n",
    "print('MNIST dataset test set label dimensions:', yts_mnist.shape)\n",
    "print('\\nCIFAR dataset train set feature dimensions:', xtr_cifar.shape)\n",
    "print('CIFAR dataset test set feature dimensions:', xts_cifar.shape)\n",
    "print('CIFAR dataset test set label dimensions:', str_cifar.shape)\n",
    "print('CIFAR dataset test set label dimensions:', yts_cifar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"experiments\"></a>\n",
    "## 4. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use libsvm-weights-3.23 to train our model (https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/weights/libsvm-weights-3.23.zip)\n",
    "\n",
    "**Instrunction of how to use libsvm**: \n",
    "\n",
    "### `svm-train` set params\n",
    "```\n",
    "Usage: svm-train [options] training_set_file [model_file]\n",
    "options:\n",
    "-s svm_type : set type of SVM (default 0)\n",
    "        0 -- C-SVC            (multi-class classification)\n",
    "        1 -- nu-SVC           (multi-class classification)\n",
    "        2 -- one-class SVM\n",
    "        3 -- epsilon-SVR      (regression)\n",
    "        4 -- nu-SVR           (regression)\n",
    "-t kernel_type : set type of kernel function (default 2)\n",
    "        0 -- linear: u'*v\n",
    "        1 -- polynomial: (gamma*u'*v + coef0)^degree\n",
    "        2 -- radial basis function: exp(-gamma*|u-v|^2)\n",
    "        3 -- sigmoid: tanh(gamma*u'*v + coef0)\n",
    "        4 -- precomputed kernel (kernel values in training_set_file)\n",
    "-c cost : set the parameter C of C-SVC, epsilon-SVR, and nu-SVR (default 1)\n",
    "-b probability_estimates : whether to train a SVC or SVR model for probability estimates, 0 or 1 (default 0)\n",
    "-v n: n-fold cross validation mode\n",
    "-q : quiet mode (no outputs)\n",
    "```\n",
    "More details: https://github.com/cjlin1/libsvm/blob/master/README#L134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(y, X, model, options='-b 1 -q'):\n",
    "    \"\"\"\n",
    "    y: a list/tuple/ndarray of labels\n",
    "    X: a list/tuple of training instances\n",
    "    model: trained SVM model\n",
    "    \n",
    "    p_labels: a list of predicted labels\n",
    "    p_acc: a tuple including  accuracy (for classification), mean-squared\n",
    "           error, and squared correlation coefficient (for regression).\n",
    "    p_vals: a list of decision values or probability estimates (if '-b 1'\n",
    "            is specified). If k is the number of classes, for decision values,\n",
    "            each element includes results of predicting k(k-1)/2 binary-class\n",
    "            SVMs. For probabilities, each element contains k values indicating\n",
    "            the probability that the testing instance is in each class.\n",
    "            Note that the order of classes here is the same as 'model.label'\n",
    "            field in the model structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    p_labels, p_acc, p_vals = svm_predict(y, X, model, options)\n",
    "    ACC, MSE, SCC = evaluations(y, p_labels)\n",
    "    return ACC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"baseline\"></a>\n",
    "### 4.1. Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test set of original svm model trained on Fashion-MNIST: 86.900\n",
      "CPU times: user 2min 24s, sys: 2.89 s, total: 2min 27s\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set W = [] if no weights are given\n",
    "# Train a base svm and get baseline test accuracy for Fashion-MNIST dataset\n",
    "model_mnist = svm_train([], str_mnist, xtr_mnist.tolist(), '-b 1')\n",
    "ACC_mnist = evaluate_prediction(yts_mnist, xts_mnist, model_mnist)\n",
    "\n",
    "print('Accuracy of test set of original svm model trained on Fashion-MNIST: %.3f' %ACC_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test set of original svm model trained on CIFAR: 73.350\n",
      "CPU times: user 3min 46s, sys: 4.01 s, total: 3min 50s\n",
      "Wall time: 4min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train a base svm and get baseline test accuracy for CIFAR dataset\n",
    "model_cifar = svm_train([], str_cifar, xtr_cifar.tolist(), '-b 1')\n",
    "ACC_cifar = evaluate_prediction(yts_cifar, xts_cifar, model_cifar)\n",
    "\n",
    "print('Accuracy of test set of original svm model trained on CIFAR: %.3f' %ACC_cifar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rw_method1\"></a>\n",
    "### 4.2. Importance Reweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rho_0 and rho_1 are given \n",
    "rho0 = 0.2\n",
    "rho1 = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\beta(x,y) = \\frac{p_{\\rho}(S=y|X=x)-\\rho_{1-y}}{(1-\\rho_{0}-\\rho_{1})p_{\\rho}(S=y|X=x)},\n",
    "\\end{equation}\n",
    "    where $p_{\\rho}(X,Y)$ can be estimated from noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateBeta(S, prob, rho0, rho1):\n",
    "    '''\n",
    "    *Adapted from tutorial 11\n",
    "    input:\n",
    "    S - Str, label with noise\n",
    "    prob - probablity returned from trained svm\n",
    "    rho0 - given value, which is 0.2\n",
    "    rho1 - given value, which is 0.4\n",
    "    \n",
    "    return:\n",
    "    beta - 1d array, weight\n",
    "    '''\n",
    "    beta = np.zeros((len(S),1)) # Initialise beta\n",
    "    for i in range(len(S)):    \n",
    "        if S[i] == 1:\n",
    "            beta[i] = (prob[i][1]-rho0)/((1-rho0-rho1)*prob[i][1]+1e-5)\n",
    "        else:\n",
    "            beta[i] = (prob[i][0]-rho1)/((1-rho0-rho1)*prob[i][0]+1e-5)\n",
    "            \n",
    "    # remove negative weights\n",
    "    # below is a simple condition to ensure convexity of weight beta\n",
    "    for j in range(len(beta)):\n",
    "        if beta[j] < 0:\n",
    "            beta[j] = 0.0\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_1(dataset_xtr, dataset_str, dataset_xts, dataset_yts, loop=10):\n",
    "    '''\n",
    "    Evaluation for \"Importance Reweighting\" model\n",
    "    input:\n",
    "    dataset_xtr: training set features\n",
    "    dataset_str: training set labels\n",
    "    dataset_xts: test set features\n",
    "    dataset_yts: test set labels \n",
    "    '''    \n",
    "    \n",
    "    ACC_test_total = []\n",
    "    \n",
    "    for i in range(loop):\n",
    "        # independently and randomly sample 8000 examples from train set for training model\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(dataset_xtr, dataset_str, \\\n",
    "                         test_size=0.2, random_state=0, shuffle=True)\n",
    "        \n",
    "        train_data, vali_data = X_train.tolist(), X_test.tolist()\n",
    "        train_class, vali_class = y_train, y_test\n",
    "        \n",
    "        # test data\n",
    "        test_data = dataset_xts.tolist()\n",
    "      \n",
    "        # Train SVM model\n",
    "        model_train = svm_train([],train_class, train_data,'-b 1')\n",
    "        \n",
    "        # Compute probability, accuracy \n",
    "        p_label, p_acc, probS = svm_predict(train_class, train_data, model_train, '-b 1 -q')\n",
    "\n",
    "        # ****************** Importance reweighting *****************************\n",
    "        \n",
    "        weights = estimateBeta(train_class, probS, rho0, rho1) # Estimate beta and compute beta\n",
    "        \n",
    "        # Apply beta to the loss and train the model again \n",
    "        clf_svm = svm_train(weights, train_class, train_data, '-b 1')\n",
    "        ACC_test = evaluate_prediction(dataset_yts, test_data, clf_svm)\n",
    "\n",
    "        print('Loop %d, Acc on test set: %.3f' % (i+1, ACC_test))     \n",
    "        # Append accuracy result on test\n",
    "        ACC_test_total.append(ACC_test)\n",
    "\n",
    "    # Print evaluation results with average accuracy and std on test data\n",
    "    print('\\nPerformance evaluation for \"Importance Reweighting\": %.3f +/- %.3f' % (np.mean(ACC_test_total),\n",
    "                                                                                    np.std(ACC_test_total)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop 1, Acc on test set: 93.550\n",
      "Loop 2, Acc on test set: 93.500\n",
      "Loop 3, Acc on test set: 93.500\n",
      "Loop 4, Acc on test set: 93.500\n",
      "Loop 5, Acc on test set: 93.500\n",
      "Loop 6, Acc on test set: 93.550\n",
      "Loop 7, Acc on test set: 93.500\n",
      "Loop 8, Acc on test set: 93.550\n",
      "Loop 9, Acc on test set: 93.500\n",
      "Loop 10, Acc on test set: 93.500\n",
      "\n",
      "Performance evaluation for \"Importance Reweighting\": 93.515 +/- 0.023\n",
      "CPU times: user 23min 43s, sys: 21.6 s, total: 24min 5s\n",
      "Wall time: 25min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate_model_1(xtr_mnist, str_mnist, xts_mnist, yts_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop 1, Acc on test set: 80.200\n",
      "Loop 2, Acc on test set: 80.200\n",
      "Loop 3, Acc on test set: 80.150\n",
      "Loop 4, Acc on test set: 79.950\n",
      "Loop 5, Acc on test set: 79.950\n",
      "Loop 6, Acc on test set: 79.850\n",
      "Loop 7, Acc on test set: 80.350\n",
      "Loop 8, Acc on test set: 79.950\n",
      "Loop 9, Acc on test set: 80.300\n",
      "Loop 10, Acc on test set: 80.350\n",
      "\n",
      "Performance evaluation for \"Importance Reweighting\": 80.125 +/- 0.176\n",
      "CPU times: user 46min 40s, sys: 45.4 s, total: 47min 26s\n",
      "Wall time: 50min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate_model_1(xtr_cifar, str_cifar, xts_cifar, yts_cifar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rw_method2\"></a>\n",
    "### 4.3. Reweighting method by Nagarajan Natarajan\n",
    "(https://www.cs.cmu.edu/~pradeepr/paperz/learning_nl_nips.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(x, y):\n",
    "    # seperate label of 0 and 1\n",
    "    z = list(zip(x,y))\n",
    "    # sort label, used for later weights multiplication  \n",
    "    x_,y_ = zip(*sorted(z,key=lambda x:x[1])) \n",
    "    return x_,y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_2(dataset_xtr, dataset_str, dataset_xts, dataset_yts, loop=10):\n",
    "    # Create empty list\n",
    "    ACC_test_total = []\n",
    "    \n",
    "    for i in range(loop):\n",
    "        \n",
    "        # independently and randomly sample 8000 examples from train set for training model\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(dataset_xtr, dataset_str, test_size=0.2, \\\n",
    "                         random_state=0, shuffle=True)\n",
    "        \n",
    "        train_data, train_class = sort_data(X_train, y_train)\n",
    "        vali_data, vali_class = sort_data(X_test,y_test)\n",
    "        \n",
    "        # test data\n",
    "        test_data = dataset_xts.tolist()\n",
    "        \n",
    "        # Get the number of label == 0 and number of label == 1    \n",
    "        num_0 = collections.Counter(train_class)[0]\n",
    "        num_1 = collections.Counter(train_class)[1]\n",
    "        # Construct weight list\n",
    "        #alpha = (0.5-rho0)/(1-rho0-rho1)\n",
    "        alpha = (1-rho1+rho0)/2\n",
    "        A = list(np.ones(num_0)*alpha) + list(np.ones(num_1)*(1-alpha))        \n",
    "#-------------------------------------------------------------------------------------     \n",
    "        # Train SVM model\n",
    "        model_train = svm_train(A, train_class, train_data, '-b 1')\n",
    "        ACC_test = evaluate_prediction(dataset_yts, test_data, model_train)\n",
    "        \n",
    "        print('Fold: % d, Acc on test set: %.3f' % (i+1, ACC_test))          \n",
    "        # Append data\n",
    "        ACC_test_total.append(ACC_test)\n",
    "    # Print 5-cv results with mean and std\n",
    "    print('\\n5-fold CV accuracy: %.3f +/- %.3f' % (np.mean(ACC_test_total), \n",
    "                                                  np.std(ACC_test_total)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1, Acc on test set: 90.750\n",
      "Fold:  2, Acc on test set: 90.700\n",
      "Fold:  3, Acc on test set: 90.700\n",
      "Fold:  4, Acc on test set: 90.700\n",
      "Fold:  5, Acc on test set: 90.700\n",
      "Fold:  6, Acc on test set: 90.700\n",
      "Fold:  7, Acc on test set: 90.700\n",
      "Fold:  8, Acc on test set: 90.700\n",
      "Fold:  9, Acc on test set: 90.750\n",
      "Fold:  10, Acc on test set: 90.700\n",
      "\n",
      "5-fold CV accuracy: 90.710 +/- 0.020\n",
      "CPU times: user 14min 48s, sys: 9.13 s, total: 14min 57s\n",
      "Wall time: 15min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate_model_2(xtr_mnist, str_mnist, xts_mnist, yts_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1, Acc on test set: 77.300\n",
      "Fold:  2, Acc on test set: 77.350\n",
      "Fold:  3, Acc on test set: 77.300\n",
      "Fold:  4, Acc on test set: 77.400\n",
      "Fold:  5, Acc on test set: 77.000\n",
      "Fold:  6, Acc on test set: 77.050\n",
      "Fold:  7, Acc on test set: 76.900\n",
      "Fold:  8, Acc on test set: 77.150\n",
      "Fold:  9, Acc on test set: 77.300\n",
      "Fold:  10, Acc on test set: 76.800\n",
      "\n",
      "5-fold CV accuracy: 77.155 +/- 0.197\n",
      "CPU times: user 22min 45s, sys: 13.8 s, total: 22min 59s\n",
      "Wall time: 24min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate_model_2(xtr_cifar, str_cifar, xts_cifar, yts_cifar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"estimate_rho\"></a>\n",
    "### 4.4. Estimate noise rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_noise_rate(prob, y_noise):\n",
    "    prob_1 = [prob[i][0] for i in range(len(prob)) if y_noise[i] == 0]\n",
    "    prob_0 = [prob[i][1] for i in range(len(prob)) if y_noise[i] == 1]\n",
    "\n",
    "    top_10_idx_1 = np.argsort(np.array(prob_1))[:10]\n",
    "    top_10_values_1 = [prob_1[i] for i in top_10_idx_1]\n",
    "    \n",
    "    top_10_idx_0 = np.argsort(np.array(prob_0))[:10]\n",
    "    top_10_values_0 = [prob_0[i] for i in top_10_idx_0]\n",
    "    \n",
    "    return np.mean(top_10_values_1), np.mean(top_10_values_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_rho(X, y, loop=5):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    X - array-like, shape = (n_samples, n_features)\n",
    "    y - array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
    "    loop- int, looping times [Optional]\n",
    "    return:\n",
    "    estimated rho0 - scalar\n",
    "    estimated rho1 - scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    estimated_rho0 = []\n",
    "    estimated_rho1 = []\n",
    "\n",
    "    for i in range(loop):\n",
    "        print('\\nLoop %d' %(i+1))\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        train_data = X_train.tolist() # change array type to list\n",
    "        train_label = y_train\n",
    "        W = np.ones(len(train_label))\n",
    "        %%time\n",
    "        # Train C-SVC SVM model\n",
    "        model_train = svm_train(W, train_label, train_data, '-b 1 -s 0 -q')\n",
    "        p_label, p_acc, p_val = svm_predict(train_label, train_data, model_train, '-b 1')\n",
    "        rho1,rho0 = estimate_noise_rate(p_val,y_train)\n",
    "        estimated_rho0.append(rho0)\n",
    "        estimated_rho1.append(rho1)\n",
    "\n",
    "    print('\\nestimated noise rate of rho1 is %.3f +/- %.3f' %(np.mean(estimated_rho1),\n",
    "                                                            np.std(estimated_rho1)))\n",
    "    print('estimated noise rate of rho0 is %.3f +/- %.3f' %(np.mean(estimated_rho0),\n",
    "                                                            np.std(estimated_rho0)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimate noise rate on Fashion-MNIST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loop 1\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.72 µs\n",
      "Accuracy = 69.4625% (5557/8000) (classification)\n",
      "\n",
      "Loop 2\n",
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n",
      "Accuracy = 69.475% (5558/8000) (classification)\n",
      "\n",
      "Loop 3\n",
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.2 µs\n",
      "Accuracy = 69.45% (5556/8000) (classification)\n",
      "\n",
      "Loop 4\n",
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n",
      "Accuracy = 69.45% (5556/8000) (classification)\n",
      "\n",
      "Loop 5\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.96 µs\n",
      "Accuracy = 69.45% (5556/8000) (classification)\n",
      "\n",
      "estimated noise rate of rho1 is 0.266 +/- 0.001\n",
      "estimated noise rate of rho0 is 0.160 +/- 0.001\n",
      "CPU times: user 6min 18s, sys: 1.31 s, total: 6min 19s\n",
      "Wall time: 6min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimate_rho(xtr_mnist, str_mnist, loop=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimate noise rate on CIFAR Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loop 1\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.96 µs\n",
      "Accuracy = 76.7375% (6139/8000) (classification)\n",
      "\n",
      "Loop 2\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.77 µs\n",
      "Accuracy = 77.0375% (6163/8000) (classification)\n",
      "\n",
      "Loop 3\n",
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n",
      "Accuracy = 76.7% (6136/8000) (classification)\n",
      "\n",
      "Loop 4\n",
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 6.91 µs\n",
      "Accuracy = 76.9875% (6159/8000) (classification)\n",
      "\n",
      "Loop 5\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n",
      "Accuracy = 76.8% (6144/8000) (classification)\n",
      "\n",
      "estimated noise rate of rho1 is 0.251 +/- 0.005\n",
      "estimated noise rate of rho0 is 0.199 +/- 0.003\n",
      "CPU times: user 9min 42s, sys: 1.58 s, total: 9min 44s\n",
      "Wall time: 9min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimate_rho(xtr_cifar, str_cifar, loop=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "- https://www.python.org/dev/peps/pep-0008/\n",
    "- https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/weights/libsvm-weights-3.23.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rho_0 and rho_1 are given \n",
    "rho0 = 0.2\n",
    "rho1 = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateBeta(S, prob, rho0, rho1):\n",
    "    '''\n",
    "    *Adapted from tutorial 11\n",
    "    input:\n",
    "    S - Str, label with noise\n",
    "    prob - probablity returned from trained svm\n",
    "    rho0 - given value, which is 0.2\n",
    "    rho1 - given value, which is 0.4\n",
    "    \n",
    "    return:\n",
    "    alpha - 1d array, weight\n",
    "    '''\n",
    "    alpha = np.zeros((len(S),1)) # Initialise beta\n",
    "    for i in range(len(S)):    \n",
    "        if S[i] == 1:\n",
    "            alpha[i] = (prob[i][1](rho0+rho1)-rho0)/((1-rho0-rho1)+1e-5)\n",
    "        else:\n",
    "            alpha[i] = (prob[i][0](rho0+rho1)-rho1)/((1-rho0-rho1)+1e-5)\n",
    "            \n",
    "    # remove negative weights\n",
    "    # below is a simple condition to ensure convexity of weight beta\n",
    "    for j in range(len(alpha)):\n",
    "        if alpha[j] < 0:\n",
    "            alpha[j] = 0.0\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_3(dataset_xtr, dataset_str, dataset_xts, dataset_yts, loop=10):\n",
    "    '''\n",
    "    Evaluation for \"Importance Reweighting\" model\n",
    "    input:\n",
    "    dataset_xtr: training set features\n",
    "    dataset_str: training set labels\n",
    "    dataset_xts: test set features\n",
    "    dataset_yts: test set labels \n",
    "    '''    \n",
    "    \n",
    "    ACC_test_total = []\n",
    "    \n",
    "    for i in range(loop):\n",
    "        # independently and randomly sample 8000 examples from train set for training model\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(dataset_xtr, dataset_str, \\\n",
    "                         test_size=0.2, random_state=0, shuffle=True)\n",
    "        \n",
    "        train_data, vali_data = X_train.tolist(), X_test.tolist()\n",
    "        train_class, vali_class = y_train, y_test\n",
    "        \n",
    "        # test data\n",
    "        test_data = dataset_xts.tolist()\n",
    "      \n",
    "        # Train SVM model\n",
    "        model_train = svm_train([],train_class, train_data,'-b 1')\n",
    "        \n",
    "        # Compute probability, accuracy \n",
    "        p_label, p_acc, probS = svm_predict(train_class, train_data, model_train, '-b 1 -q')\n",
    "\n",
    "        # ****************** Importance reweighting *****************************\n",
    "        \n",
    "        weights = estimateBeta(train_class, probS, rho0, rho1) # Estimate beta and compute beta\n",
    "        \n",
    "        # Apply beta to the loss and train the model again \n",
    "        clf_svm = svm_train(weights, train_class, train_data, '-b 1')\n",
    "        ACC_test = evaluate_prediction(dataset_yts, test_data, clf_svm)\n",
    "\n",
    "        print('Loop %d, Acc on test set: %.3f' % (i+1, ACC_test))     \n",
    "        # Append accuracy result on test\n",
    "        ACC_test_total.append(ACC_test)\n",
    "\n",
    "    # Print evaluation results with average accuracy and std on test data\n",
    "    print('\\nPerformance evaluation for \"Importance Reweighting\": %.3f +/- %.3f' % (np.mean(ACC_test_total),\n",
    "                                                                                    np.std(ACC_test_total)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "evaluate_model_3(xtr_mnist, str_mnist, xts_mnist, yts_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "evaluate_model_3(xtr_cifar, str_cifar, xts_cifar, yts_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
